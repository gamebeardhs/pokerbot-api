#!/usr/bin/env python3
"""
Comprehensive database analysis and optimization recommendations.
Analyzes current GTO database performance and provides scaling recommendations.
"""

import sqlite3
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
import requests

def analyze_database_performance():
    """Analyze current database performance metrics."""
    print("üîç COMPREHENSIVE DATABASE SANITY CHECK")
    print("=" * 50)
    
    # Check database file existence and size
    db_path = Path("gto_database.db")
    if db_path.exists():
        db_size_mb = db_path.stat().st_size / (1024 * 1024)
        print(f"‚úÖ Database file exists: {db_size_mb:.2f}MB")
    else:
        print("‚ùå Database file not found")
        return
    
    # Check database schema and contents
    try:
        with sqlite3.connect("gto_database.db") as conn:
            cursor = conn.cursor()
            
            # Check table structure
            cursor.execute("PRAGMA table_info(gto_situations)")
            schema = cursor.fetchall()
            print(f"‚úÖ Database schema: {len(schema)} columns")
            
            # Count total records
            cursor.execute("SELECT COUNT(*) FROM gto_situations")
            total_records = cursor.fetchone()[0]
            print(f"‚úÖ Total GTO situations: {total_records:,}")
            
            # Analyze vector dimensions
            cursor.execute("SELECT vector FROM gto_situations LIMIT 1")
            sample_vector = cursor.fetchone()
            if sample_vector:
                vector_data = np.frombuffer(sample_vector[0], dtype=np.float32)
                print(f"‚úÖ Vector dimensions: {len(vector_data)}")
            
            # Check decision distribution
            cursor.execute("SELECT recommendation, COUNT(*) FROM gto_situations GROUP BY recommendation")
            decisions = cursor.fetchall()
            print("‚úÖ Decision distribution:")
            for decision, count in decisions:
                percentage = (count / total_records) * 100
                print(f"   {decision}: {count:,} ({percentage:.1f}%)")
                
    except Exception as e:
        print(f"‚ùå Database analysis failed: {e}")

def test_api_endpoints():
    """Test all database API endpoints."""
    print("\nüß™ API ENDPOINT CONNECTIVITY TESTS")
    print("=" * 50)
    
    base_url = "http://localhost:5000/database"
    
    # Test stats endpoint
    try:
        response = requests.get(f"{base_url}/database-stats", timeout=5)
        if response.status_code == 200:
            stats = response.json()
            print(f"‚úÖ Stats endpoint: {stats['total_situations']} situations")
        else:
            print(f"‚ùå Stats endpoint failed: {response.status_code}")
    except Exception as e:
        print(f"‚ùå Stats endpoint error: {e}")
    
    # Test instant GTO endpoint with timing
    test_situations = [
        {"hole_cards": ["As", "Kh"], "position": "BTN", "pot_size": 10, "bet_to_call": 3, 
         "stack_size": 100, "num_players": 6, "betting_round": "preflop"},
        {"hole_cards": ["Qd", "Qc"], "board_cards": ["Ah", "7s", "2h"], "position": "CO", 
         "pot_size": 25, "bet_to_call": 8, "stack_size": 150, "num_players": 3, "betting_round": "flop"},
        {"hole_cards": ["9h", "9d"], "position": "UTG", "pot_size": 6, "bet_to_call": 2,
         "stack_size": 200, "num_players": 9, "betting_round": "preflop"}
    ]
    
    response_times = []
    successful_queries = 0
    
    for i, situation in enumerate(test_situations, 1):
        try:
            start_time = time.time()
            response = requests.post(f"{base_url}/instant-gto", json=situation, timeout=10)
            end_time = time.time()
            
            response_time_ms = (end_time - start_time) * 1000
            response_times.append(response_time_ms)
            
            if response.status_code == 200:
                data = response.json()
                decision = data.get("recommendation", {}).get("decision", "unknown")
                method = data.get("method", "unknown")
                print(f"‚úÖ Test {i}: {decision} ({response_time_ms:.1f}ms) via {method}")
                successful_queries += 1
            else:
                print(f"‚ùå Test {i} failed: {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå Test {i} error: {e}")
    
    if response_times:
        avg_time = sum(response_times) / len(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        
        print(f"\nüìä PERFORMANCE METRICS:")
        print(f"   Success rate: {successful_queries}/{len(test_situations)} ({(successful_queries/len(test_situations)*100):.1f}%)")
        print(f"   Average response: {avg_time:.1f}ms")
        print(f"   Fastest response: {min_time:.1f}ms") 
        print(f"   Slowest response: {max_time:.1f}ms")
        print(f"   Target: <100ms {'‚úÖ' if avg_time < 100 else '‚ö†Ô∏è'}")

def calculate_optimal_database_size():
    """Calculate optimal database size based on poker situation coverage."""
    print("\nüìà DATABASE SCALING ANALYSIS")
    print("=" * 50)
    
    # Poker situation space analysis
    positions = 9  # UTG, UTG+1, MP, MP+1, CO, BTN, SB, BB, others
    hole_cards = 169  # Unique starting hands (suited/offsuit combinations)
    betting_rounds = 4  # Preflop, flop, turn, river
    
    # Simplified situation space (conservative estimate)
    preflop_situations = positions * hole_cards * 10  # ~10 action contexts per position/hand
    postflop_multiplier = 50  # Board texture variations
    postflop_situations = preflop_situations * postflop_multiplier * 3  # 3 postflop rounds
    
    total_theoretical = preflop_situations + postflop_situations
    
    print(f"üìä POKER SITUATION SPACE ANALYSIS:")
    print(f"   Preflop situations: ~{preflop_situations:,}")
    print(f"   Postflop situations: ~{postflop_situations:,}")
    print(f"   Total theoretical: ~{total_theoretical:,}")
    
    # Practical coverage recommendations
    coverage_levels = [
        (1000, "Proof of Concept", "Basic testing, limited coverage"),
        (10000, "MVP Coverage", "Core situations, ~5% coverage"),
        (50000, "Production Ready", "Common scenarios, ~20% coverage"),
        (250000, "Professional Grade", "Comprehensive coverage, ~70% coverage"),
        (1000000, "Tournament Grade", "Near-complete coverage, ~95% coverage")
    ]
    
    print(f"\nüéØ RECOMMENDED DATABASE SIZES:")
    for size, grade, description in coverage_levels:
        coverage_pct = (size / total_theoretical) * 100
        estimated_size_mb = size * 0.002  # ~2KB per situation (vector + metadata)
        index_size_mb = size * 0.001  # HNSW index overhead
        total_mb = estimated_size_mb + index_size_mb
        
        print(f"   {size:,} situations ({grade})")
        print(f"      Description: {description}")
        print(f"      Coverage: {coverage_pct:.3f}%")
        print(f"      Storage: ~{total_mb:.1f}MB")
        print(f"      Query time: <{10 + (size/10000):.0f}ms")
        print()

def analyze_indexing_tradeoffs():
    """Analyze indexing strategy tradeoffs."""
    print("‚öñÔ∏è INDEXING STRATEGY ANALYSIS")
    print("=" * 50)
    
    strategies = [
        {
            "name": "Minimal HNSW",
            "M": 16,
            "ef_construction": 100,
            "memory_factor": 1.0,
            "query_speed": "Ultra-fast (<10ms)",
            "accuracy": "Good (85%)",
            "use_case": "Real-time gaming"
        },
        {
            "name": "Balanced HNSW", 
            "M": 32,
            "ef_construction": 200,
            "memory_factor": 1.5,
            "query_speed": "Fast (<50ms)",
            "accuracy": "Very Good (92%)",
            "use_case": "Production advisory"
        },
        {
            "name": "High-Accuracy HNSW",
            "M": 64, 
            "ef_construction": 400,
            "memory_factor": 2.0,
            "query_speed": "Moderate (<100ms)",
            "accuracy": "Excellent (97%)",
            "use_case": "Professional analysis"
        }
    ]
    
    print("üîß HNSW Configuration Recommendations:")
    for strategy in strategies:
        print(f"\n   {strategy['name']}:")
        print(f"      M (connections): {strategy['M']}")
        print(f"      ef_construction: {strategy['ef_construction']}")
        print(f"      Memory usage: {strategy['memory_factor']}x base")
        print(f"      Query speed: {strategy['query_speed']}")
        print(f"      Accuracy: {strategy['accuracy']}")
        print(f"      Best for: {strategy['use_case']}")

def provide_scaling_recommendations():
    """Provide specific scaling recommendations."""
    print("\nüöÄ SCALING RECOMMENDATIONS")
    print("=" * 50)
    
    print("IMMEDIATE ACTIONS (Current System):")
    print("‚úÖ Increase database to 10,000 situations for MVP coverage")
    print("‚úÖ Implement batch population with progress tracking")
    print("‚úÖ Add query performance monitoring and alerts")
    print("‚úÖ Create database backup and recovery procedures")
    
    print("\nSHORT-TERM OPTIMIZATIONS (1-2 weeks):")
    print("üìà Scale to 50,000 situations for production readiness")
    print("üîß Optimize HNSW parameters based on query patterns")
    print("‚ö° Add database connection pooling and caching")
    print("üìä Implement real-time performance dashboards")
    
    print("\nLONG-TERM SCALING (1-3 months):")
    print("üèóÔ∏è Distributed database across multiple nodes")
    print("üß† Implement adaptive indexing based on usage patterns")
    print("üîÑ Add automatic database updating with new GTO solutions")
    print("üéØ Achieve 250,000+ situations for professional-grade coverage")
    
    print("\nPERFORMANCE TARGETS:")
    print("‚Ä¢ Query Response: <50ms average, <100ms P95")
    print("‚Ä¢ Coverage: >90% of common poker situations")
    print("‚Ä¢ Accuracy: >95% similarity matching precision")
    print("‚Ä¢ Availability: 99.9% uptime with failover")

if __name__ == "__main__":
    analyze_database_performance()
    test_api_endpoints()
    calculate_optimal_database_size()
    analyze_indexing_tradeoffs()
    provide_scaling_recommendations()